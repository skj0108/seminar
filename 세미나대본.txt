1. RNN 순환 신경망에 대해 발표 시작하겠습니다.
3.먼저 순차데이터를 설명하자면 DNA 염기서열처럼
gat aaat 이런식으로 순서의 의미가 있고, 순서가
달리지면 기존의 의미가 손상되는 데이터를 순차
데이터라고 합니다.

보이는 것처럼 여기에 시간적 의미가 추가되면 
temporal sequence라고 하며, 이 시간이 일정한 간격을 가지고 있으면 time series라고 합니다.

4.
저희가 데이터를 다룰때 
temporal sequence를 사용하는 것보다 당연히 일정한 시간차를 가지고 있는 time series를 다루는게 더 편하겠죠. 그래서 데이터를 변환하는 과정이 필요합니다. 사진을 보시면 검정색 점들이 저희가 취득한 데이터라고 합시다. 여기서 time seires로 변환하려면 일정한 간격에 데이터를 얻어야겠죠? 
 
취득한 데이터 사이에 데이터를 추정하여 얻는 것을 보간법이라고 하며 보간법에는 다양한 방법이 있습니다. 가장 성능이 좋고 대표하는 방법은 각각의 두 점들 사이에 매끄럽게 연결된 3차식 함수를 얻어내여 원하는 위치에 데이터를 보간하는 3차 스플라인 방법입니다. 
이런식으로 균일 시간 간격의 데이터를 얻어 변환하는 방법을 resampling이라고 합니다.

5.
이 순차 데이터가 입력값으로 들어가 특정 프로세서를 거쳐 출력하는 것을 볼 수 있습니다.

그러면 이러한 순차 데이터들을 어떻게 학습하고 처리할 수 있는지에 대해 알아보겠습니다.

6. 
지금까지 배운 CNN이나, 심층 신경망은 이전 입력을 기억하지 않는 무기억 시스템이였습니다.
하지만 이런식으로 하나하나 순차 데이터가 입력되고  원하는 출력을 내주기 위해서는 입력된 데이터들을 모두 기억해야합니다. 따라서 순차데이터를 다루는 네트워크는  기본적으로 기억 시스템을 지니고 있어야겠죠? 

7.
이 그림을 보면 기본적인 순환 신경망은 얕은 신경망구조에서 순환이 추가 됐고. 이것에 의해 출력이 이전의 모든 입력에 영향을 받습니다. 자세히 살펴보면 이부분이 얕은 신경망과 다르죠? 
수식으로 보면 입력과 가중치가 곱해지고, 이전 step의 hidden state가 또다른 가중치와 곱해져 더해지는 것을 볼 수 있습니다.  즉, 입력과 hn-1을 concatenate시켜 사용한 것이고 입력 길이가 길어지는 효과가 발생합니다.

추가로 RNN은 하이퍼볼릭 탄젠트함수를 활성함수로 주로 사용합니다.

또 이런식의 그림으로 vanilla RNN을 표현할 수 있습니다.

8.
순환 신경망도 심층 신경망처럼 여러 layer들을 쌓아올릴 수 있는데요. 이렇게 되면 입력층 뿐 아니라 모든 hidden layer의 길이가 기존보다 2배이상으로 길어지기 때문에 구조가 매우 복잡합니다. 또한 gradient도 매우 복잡하고 많은 곳을 뚫고 전달해야하기 때문에 학습이 잘 되지 않아, 권장되지 않습니다.

9.
vanilla RNN도 마찬가지로 많이 사용되지 않는데요.
만약 x0를 학습시킨다고 보면 각 출력의 loss에 대한 xo의 gradient를 구해야합니다.
여기서 ht+1에 대한 x0의 gradient를 구한다고 하면
두 step사이에 많이 차이가 있을 경우 기울기가 점점 소실되어 학습능력이 떨어질 것입니다.
따라서 이를 개선한 네트워크들을 알아보겠습니다.

10.
첫번째로는 기억할 것은 오래 기억하고, 잊을 것은 빨리 잊어버리자는 LSTM이 있습니다.

vanilla RNN과 크게 다른점은 여기 보시다시피 한줄이 있지만 여기에는 두개의 line이 있습니다.

11.
선이 두개인 이유는 기억을 오랫동안 유지할 수 있고 새로운 특징을  앞에서 배웠던 Residual connection처럼 덧셈으로 받는 Cell state가 추가됐기 때문입니다.
Hidden state는 기존 RNN과 동일하게 출력과 다음 step으로 넘기는 정보입니다.
직관적으로 보셨을때 이 선에서는 gradient가 잘 타고 넘어갈 수 있겠죠?
이 복잡해보이는 구조를 하나씩 살펴보겠습니다.

12.
여기 보이는 구조는 forget gate라는 구조입니다.
여기서는 이전에서부터 기억된 특징들을 하나하나 기억할지 말지를 결정하는 gate입니다.
입력값과 이전 step의 hidden state를 fully connected한 후 활성함수로 sigmoid를 사용해서 출력값을 0~1로 만들어주고 cell state에 특징별로 곱을 합니다.
특징별로 0에 가까운 값을 곱하면 이전 기억이 많이 잊게 되고 1에 가까운 값을 곱하면 기억에 조금 더 남게 될 것입니다.



13.
여기 Input gate도 마찬가지로 입력값과 이전 hidden state를 fully-connected하고 시그모이드를 거칩니다. 그러면 당연히 출력값은 0에서 1을 가지겠죠?
빨간 선을 보시면 vanilla RNN처럼 두개를 이용해 새로운 특징을 뽑는데요 그 이후 input gate에서 얻은 0에서 1값을 특징별로 곱해주어 Cell state에 더하게 됩니다.
이 의미는 input gate에서 기억을 전담하고 잇는 Cell state에 새롭게 추출한 특징들을 얼마나 더해줄지 즉 얼마나 기억할지를 결정해줍니다.

14.
앞에서 알아봤던 forget gate와 input gate를 이용하여 cell state 관점에서 보겠습니다.
이전 cell state에서 forget gate를 지나 잊을것은 잊게 되었죠. 그 후 새롭게 추출한 특징들이 input gate와 곱하고 cell state에 더해져 기억하고 싶은 새로운 특징들만 기억하게되었습니다. 

이번 step의 cell state 수식을 보면 저 기호는 아다마르 연산을 나타내고 벡터의 같은 위치 원소별 곱을 
의미합니다. 아다마르 연산자를 통해 특징별로 기억하고, 잊고, 새로운 정보를 받을 수 있습니다.

15. 
다음으로는 output gate입니다.
여기도 두개를 fully-connected 후 sigmoid거치게 되어 0에서 1의 출력값을 가집니다. 

여기 나와있듯이 output gate는 cell state 중 어떤 특징을 출력하지 결정하는 역할을 가지는데요. 그 이유를 Hidden state 관점에서 알아보겠습니다.

16.
앞에서 cell state 계산을 알아보았죠?
hidden state는 cell state 값을 우선 하이퍼볼릭 탄젠트함수를 거쳐 bound 후 output gate의 값을 곱해줍니다. 역시 아다마르 연산자를 통해 특징별로 계산할 수 있습니다.

여기를 살펴보면 뜬금없이 하이퍼볼릭 탄젠트 함수가 나왔는데요. 이부분은 다른 곳과 다르게 fully-connected가 아니라 단순히 활성함수만 거치는 구조입니다. 거치게 된 이유는 원래여기서 -1~1값을 가지고 있었는데 0~1을 곱해줘도 -1~1값을 유지합니다. 하지만 여기서 -1~1값을 추가로 더해주기때문에 cell state의 값이 -2~2의 범위로 증가됩니다. 
여러 step을 거치게되면 이 값이 매우 커지겠죠

점점 커지게되어 gradient 커지면 가중치들이 비정상적으로 큰값이 되어 발산하는 gradient explode 현상이 발생할 수 있습니다

따라서 하이퍼볼릭 탄젠트함수를 통해 -1~1값으로 bound한 후 output gate를 통해 출력 특징을 선별하는 것입니다.

17. 
또다른 구조로는  LSTM을 간소화 한 GRU라는 구조가 있습니다.
LSTM과 가장 큰 차이점은 Cell state가 없고 hidden state만 존재합니다. 
또한 forget gate와 input gate가 결합되고 reset gate가 추가됐습니다.
이 구조도 차근차근 알아보겠습니다.

18. 
GRU는 forget gate와 input gate가 결합됐다고 했습니다. 그 이유는 LSTM의 forget gate와 동일하지만, 여기를 살펴보면 forget gate에 나온 값에서 1을 빼서 input gate로 사용합니다.
한마디로 forget gate로 인해 잊어버린 부분을 새로운 feature로 채운다는 의미죠.

19.
이 구조는 LSTM에는 보이지 않았죠?
이것도 앞에서 배웠던 gate들과 마찬가지로 입력값과 이전의 hidden state를 결합하여 sigmoid를 거칩니다. 따라서 0~1의 값을 가겠죠.

이 게이트의 역할은 현재 step에서 feature를 뽑을 때 이전 기억을 얼만큼 잊을지를 결정합니다.
그 이유는 Hidden state 관점에서 살펴보겠습니다.

20.
새로운 피처 gt를 뽑을 때 입력값과 이전 hidden state를 그대로 사용하는 것이 아니라 reset gate와 ht-1을 곱하여 입력값과 함께 결합합니다. 

여기서 forget gate와 reset gate의 의미가 조금 헷갈릴 수 있는데 이전 기억에서 기억할 부분을 결정하는 것은 forget gate, 새로운 특징을 뽑을때 이전 hidden state를 얼마나 사용할지는 reset gate입니다.

hidden state는 forget gate에서 제어된만큼 넘어오고 forget gate에 의해 상보적인 만큼 새로운 feature를 입력받아 다음 출력으로 나아갑니다.

상보적으로 잊어버린 만큼만 새로운 기억이 추가되기 때문에 -1~1의 범위로 유지되고 LSTM같이 하이퍼볼릭 탄젠트함수를 이용하지 않아도 됩니다.

21. 
이번에는 RNN 입출력 텐서에 대해 알아보겠습니다.
먼저 입력텐서를 보면 
batch dimension이 먼저 sequence 길이의 demension, feature벡터의 index dimesion 순서로 이루어집니다.

순차데이터를 여러개 한번에 학습할 때는 입출력 길이가 데이터마다 다른데 입력에 경우 L보다 짧을 경우 앞을 0으로 채웁니다.

그 이유는 마지막 입력 후 출력이 진행되기 때문입니다.

22. 
RNN의 출력텐서도 입력텐서와 마찬가지로 차원 순서는 동일하지만 길이가 짧을경우 앞이아닌 뒤를 0으로 채웁니다.
그 이유는 출력벡터 시작은 항상 출력이 시작된 곳에서 시작을 하기에 시점이 고정되어있기 때문입니다.

23. 
RNN에서 back propagation을 진행할 때 다른 역전파들과 동일하게 펼쳐 둔 상태에서 진행합니다. 다른점은 시간적으로 펼쳐졌기 때문에 각 과정의 펼쳐진 trainable variables 모두 동일합니다.

그러면 실제로 학습에는 어떻게 적용되는지 알아보겠습니다.

24.
RNN도 한번에 입력하기 어려운 큰 데이터를 batch로 쪼개 입력할 수 있습니다.

25.
하지만 시간펼침 역전파는 시간적으로도 펼치고, batch로도 펼쳐야합니다. 따라서 기존 역전파와 다르게 역전파를 위한 추가적인 메모리가 필요하므로.
다른 신경망들에 비해 batch를 키우는 것이 매우 어렵죠.

26.
다중입력, 다중 출력의 경우는 truncated bptt를 이용해 메모리 문제를 해결할 수 있는데, truncation은 어느 선에서 잘라주는 의미를 가지고 있습니다.

27. 
말 그대로 순차 데이터의 길이를 일정한 T 길이로 잘라 배치를 나누는 것처럼 한번에 계산하는 크기를 줄여줍니다.

28.
이런식으로 길이 L의 입력을 T로 쪼개어 순서대로 학습을 합니다. 또한 이 안에서만 역전파가 진행되므로 한번에 역전파하는 길이가 제한되어 사용하는 메모리를 줄일 수 있습니다. 

29.
하지만 이 안에서만 역전파가 진행되기 때문에
time step이 T이상 떨어지면 gradient를 전달할 수  없어 역전파가 이루어지지 않습니다. 즉 T이상 떨어진 입출력에서는 학습을 할 수 없습니다.

이러한 문제점을 생각하여 학습하고자 하는 부분이 어느정도 시간차이가 있는지 고려하여 적절한T를 설정해야합니다.

만약 시간차이가 많이나는 입출력사이에 gradient를 전달해야한다면 batch size를 극단적으로 낮추고 최대한 큰 gpu를 사용하는 방법밖에 없습니다.




























